{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Copy_of_Fine_tune_ALBERT_sentence_pair_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "42-lJ1u9IHT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f48a5e-05d5-4ba0-e6c7-3e3891eaeb20"
      },
      "source": [
        "!pip install datasets==1.0.1\n",
        "!pip install transformers==3.1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==1.0.1\n",
            "  Downloading datasets-1.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (0.3.4)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (4.64.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (1.3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (3.6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.1) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.1) (1.15.0)\n",
            "Installing collected packages: xxhash, datasets\n",
            "Successfully installed datasets-1.0.1 xxhash-3.0.0\n",
            "Collecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.21.6)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 23.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.1.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE_TpNaSZQ5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b7236b-ae44-4338-fcb4-e0dd972f733a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "! pip install wiktionaryparser\n",
        "from wiktionaryparser import WiktionaryParser\n",
        "parser = WiktionaryParser()\n",
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wiktionaryparser\n",
            "  Downloading wiktionaryparser-0.0.97-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wiktionaryparser) (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from wiktionaryparser) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->wiktionaryparser) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->wiktionaryparser) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->wiktionaryparser) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->wiktionaryparser) (2.10)\n",
            "Installing collected packages: wiktionaryparser\n",
            "Successfully installed wiktionaryparser-0.0.97\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 6.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch version 1.10.0+cu111 available.\n",
            "TensorFlow version 2.8.0 available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GFpNwk8R8TQX",
        "outputId": "bad7f56d-5871-432d-813e-36e164123aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCA-bKfuEbjH"
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/CSCI-544/pnli_train.csv', header=None,  encoding=\"UTF-8\")\n",
        "df_val   = pd.read_csv('/content/drive/MyDrive/CSCI-544/pnli_dev.csv', header=None, encoding=\"UTF-8\")\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/CSCI-544/pnli_test_unlabeled.csv', header=None, encoding=\"UTF-8\")\n",
        "df_train.columns = [\"sentence1_unprocessed\",\"sentence2_unprocessed\",\"label\"]\n",
        "df_val.columns = [\"sentence1_unprocessed\",\"sentence2_unprocessed\",\"label\"]\n",
        "df_test.columns = [\"sentence1_unprocessed\",\"sentence2_unprocessed\"]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre Processed Text\n",
        "\n",
        "# df_train = pd.read_csv('/content/drive/MyDrive/CSCI-544/trainpy.csv', header=0,  encoding=\"UTF-8\")\n",
        "# df_val   = pd.read_csv('/content/drive/MyDrive/CSCI-544/valpy.csv', header=0, encoding=\"UTF-8\")\n",
        "# df_test = pd.read_csv('/content/drive/MyDrive/CSCI-544/testpy.csv', header=0, encoding=\"UTF-8\")"
      ],
      "metadata": {
        "id": "g92bmc-oqeKT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNs2FWNJSLSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cedd50-c941-4e01-b3dd-9062a292a000"
      },
      "source": [
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5983, 3)\n",
            "(1055, 3)\n",
            "(4850, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irj7itV0UCF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "183f2094-98aa-4220-bc35-c8ec7e289bcb"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               sentence1_unprocessed  \\\n",
              "0                             Sometimes do exercise.   \n",
              "1                               Who eats junk foods.   \n",
              "2                                  A person is sick.   \n",
              "3                                  A person is dead.   \n",
              "4  A person eats properly and do exercise regularly.   \n",
              "\n",
              "                     sentence2_unprocessed  label  \n",
              "0  A person typically desire healthy life.      1  \n",
              "1  A person typically desire healthy life.      0  \n",
              "2  A person typically desire healthy life.      1  \n",
              "3  A person typically desire healthy life.      0  \n",
              "4  A person typically desire healthy life.      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13cdfacc-ddcb-4ef0-acf8-e61112e60f0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1_unprocessed</th>\n",
              "      <th>sentence2_unprocessed</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sometimes do exercise.</td>\n",
              "      <td>A person typically desire healthy life.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who eats junk foods.</td>\n",
              "      <td>A person typically desire healthy life.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A person is sick.</td>\n",
              "      <td>A person typically desire healthy life.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A person is dead.</td>\n",
              "      <td>A person typically desire healthy life.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A person eats properly and do exercise regularly.</td>\n",
              "      <td>A person typically desire healthy life.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13cdfacc-ddcb-4ef0-acf8-e61112e60f0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13cdfacc-ddcb-4ef0-acf8-e61112e60f0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13cdfacc-ddcb-4ef0-acf8-e61112e60f0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyenchant\n",
        "# import enchant\n"
      ],
      "metadata": {
        "id": "xIDqmCm1wNxz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(sentence):\n",
        "  stop_list = [\"a\",\"an\",\"the\",\"typically\",\"is\",\"for\",\"they\",\"their\",\"i\", \"them\",\"themselves\"]\n",
        "  new_sentence = \"\"\n",
        "  for word in sentence.split():\n",
        "    if word.lower() not in stop_list:\n",
        "      new_sentence = new_sentence + \" \" + word\n",
        "  return new_sentence.strip()\n",
        "\n",
        "def remove_un(old_sent):\n",
        "  new_sent =\"\"\n",
        "  d = enchant.Dict(\"en_US\")\n",
        "  for word in old_sent.split():\n",
        "    new_word = word\n",
        "    if not d.check(word):\n",
        "      if word.lower().startswith(\"un\") and word.lower() not in [\"union\",\"unite\",\"united\"]:\n",
        "        print(\"Old Word \" + word)\n",
        "        new_word = \"not \" + word.lower().split('un')[1]\n",
        "        print(new_word)\n",
        "    new_sent = new_sent + \" \" + new_sent\n",
        "  return new_sent\n",
        "      \n",
        "\n",
        "def correct_spelling_pycheck(old_sent):\n",
        "  old_sent = re.sub(r'[^\\w\\s]', '', old_sent)\n",
        "  new_sent =\"\"\n",
        "  for word in spell.unknown(list(old_sent.split())):\n",
        "    if word.lower() not in [\"tv\",\"php\",\"gps\", \"aseexuality\", \"angryman\", \"fastfood\",\"golddigger\",\"asexuality\",\"covid\",\"bbq\", \"vcr\"] and \"un\" not in word.lower(): \n",
        "      new_word = str(spell.correction(word))\n",
        "      new_sent = new_sent + \" \"  + new_word\n",
        "      if str(word).lower() != new_word.lower():\n",
        "        print(\"Old Word: \"+ word)\n",
        "        print(\"New Word: \"+ new_word)\n",
        "  return new_sent\n",
        "\n",
        "def data_preprocessing(curr_value):\n",
        "  new_value = str(curr_value)\n",
        "\n",
        "  #new_value = str(remove_un(new_value))\n",
        "  #print(new_value)\n",
        "  #new_value = str(correct_spelling_pycheck(new_value))\n",
        "  #print(new_value)\n",
        "\n",
        "  #print(\"\")\n",
        "  new_value = str(remove_stop_words(curr_value))\n",
        "  \n",
        "  return str(new_value)\n",
        "\n",
        "def itter_dataset(dataset, column):\n",
        "  cat_list = []\n",
        "  for index, row in dataset.iterrows():\n",
        "      wt = row[column+\"_unprocessed\"]    \n",
        "      cat = data_preprocessing(wt)\n",
        "      cat_list.append(cat)\n",
        "  #print(cat_list)\n",
        "  dataset[column+\"_3\"] = cat_list\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "aG03QtomGEPC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"sentence1\"] = df_train.apply(lambda row : data_preprocessing(row[\"sentence1_unprocessed\"]), axis=1)\n",
        "df_val[\"sentence1\"] = df_val.apply(lambda row : data_preprocessing(row[\"sentence1_unprocessed\"]), axis=1)\n",
        "#df_val = itter_dataset(df_val, \"sentence1\")\n",
        "df_test[\"sentence1\"] = df_test.apply(lambda row : data_preprocessing(row[\"sentence1_unprocessed\"]), axis=1)\n",
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhPA-Jr4bs_-",
        "outputId": "59a74d66-ea58-45cb-bfe4-199da5fe5bd7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       sentence1_unprocessed  \\\n",
            "0          A person is looking for accuracy.   \n",
            "1       A person does not care for accuracy.   \n",
            "2       The person double checks their data.   \n",
            "3  The person speeds through the experiment.   \n",
            "4                 A person is studying well.   \n",
            "\n",
            "                          sentence2_unprocessed  label  \\\n",
            "0  A person typically desires accurate results.      1   \n",
            "1  A person typically desires accurate results.      0   \n",
            "2  A person typically desires accurate results.      1   \n",
            "3  A person typically desires accurate results.      0   \n",
            "4  A person typically desires accurate results.      1   \n",
            "\n",
            "                           sentence1  \n",
            "0           person looking accuracy.  \n",
            "1     person does not care accuracy.  \n",
            "2         person double checks data.  \n",
            "3  person speeds through experiment.  \n",
            "4              person studying well.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_val.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S4BmyPbh2ur",
        "outputId": "d3aca9ab-2121-4bed-9265-0a5c082fbde8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       sentence1_unprocessed  \\\n",
            "0          A person is looking for accuracy.   \n",
            "1       A person does not care for accuracy.   \n",
            "2       The person double checks their data.   \n",
            "3  The person speeds through the experiment.   \n",
            "4                 A person is studying well.   \n",
            "\n",
            "                          sentence2_unprocessed  label  \\\n",
            "0  A person typically desires accurate results.      1   \n",
            "1  A person typically desires accurate results.      0   \n",
            "2  A person typically desires accurate results.      1   \n",
            "3  A person typically desires accurate results.      0   \n",
            "4  A person typically desires accurate results.      1   \n",
            "\n",
            "                           sentence1  \n",
            "0           person looking accuracy.  \n",
            "1     person does not care accuracy.  \n",
            "2         person double checks data.  \n",
            "3  person speeds through experiment.  \n",
            "4              person studying well.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"sentence2\"] = df_train.apply(lambda row : data_preprocessing(row[\"sentence2_unprocessed\"]), axis=1)\n",
        "df_val[\"sentence2\"] = df_val.apply(lambda row : data_preprocessing(row[\"sentence2_unprocessed\"]), axis=1)\n",
        "df_test[\"sentence2\"] = df_test.apply(lambda row : data_preprocessing(row[\"sentence2_unprocessed\"]), axis=1)"
      ],
      "metadata": {
        "id": "TdF1ZzxOcrpd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save Pre Processed Text\n",
        "# print(df_train.head())\n",
        "# df_train.to_csv(\"/content/drive/MyDrive/CSCI-544/trainpy.csv\", sep=',')\n",
        "# df_val.to_csv(\"/content/drive/MyDrive/CSCI-544/valpy.csv\", sep=',')\n",
        "# df_test.to_csv(\"/content/drive/MyDrive/CSCI-544/testpy.csv\", sep=',')\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFxgGa6ucxdt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Pre Processed Text\n",
        "\n",
        "# df_train = pd.read_csv('/content/drive/MyDrive/CSCI-544/train.csv', header=0,  encoding=\"UTF-8\")\n",
        "# df_val   = pd.read_csv('/content/drive/MyDrive/CSCI-544/val.csv', header=0, encoding=\"UTF-8\")\n",
        "# df_test = pd.read_csv('/content/drive/MyDrive/CSCI-544/test.csv', header=0, encoding=\"UTF-8\")\n",
        "# df_train[\"sentence1\"] = df_train.apply(lambda row : data_preprocessing(row[\"sentence1\"]), axis=1)\n",
        "# df_val[\"sentence1\"] = df_val.apply(lambda row : data_preprocessing(row[\"sentence1\"]), axis=1)\n",
        "# #df_val = itter_dataset(df_val, \"sentence1\")\n",
        "# df_test[\"sentence1\"] = df_test.apply(lambda row : data_preprocessing(row[\"sentence1\"]), axis=1)\n",
        "# df_train[\"sentence2\"] = df_train.apply(lambda row : data_preprocessing(row[\"sentence2\"]), axis=1)\n",
        "# df_val[\"sentence2\"] = df_val.apply(lambda row : data_preprocessing(row[\"sentence2\"]), axis=1)\n",
        "# df_test[\"sentence2\"] = df_test.apply(lambda row : data_preprocessing(row[\"sentence2\"]), axis=1)\n",
        "# print(df_val.head())\n"
      ],
      "metadata": {
        "id": "zJVABQ_i2pmC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Final training\n",
        "final_train_data = pd.concat([df_train, df_val], ignore_index=True, sort=False)\n",
        "print(final_train_data.head())\n",
        "print(final_train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv7Oa0IhDZxS",
        "outputId": "a4a6bc2d-77bc-42df-a4dc-02706ab2ec3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               sentence1_unprocessed  \\\n",
            "0                             Sometimes do exercise.   \n",
            "1                               Who eats junk foods.   \n",
            "2                                  A person is sick.   \n",
            "3                                  A person is dead.   \n",
            "4  A person eats properly and do exercise regularly.   \n",
            "\n",
            "                     sentence2_unprocessed  label  \\\n",
            "0  A person typically desire healthy life.      1   \n",
            "1  A person typically desire healthy life.      0   \n",
            "2  A person typically desire healthy life.      1   \n",
            "3  A person typically desire healthy life.      0   \n",
            "4  A person typically desire healthy life.      1   \n",
            "\n",
            "                                         sentence1  \\\n",
            "0                           Sometimes do exercise.   \n",
            "1                             Who eats junk foods.   \n",
            "2                                     person sick.   \n",
            "3                                     person dead.   \n",
            "4  person eats properly and do exercise regularly.   \n",
            "\n",
            "                     sentence2  \n",
            "0  person desire healthy life.  \n",
            "1  person desire healthy life.  \n",
            "2  person desire healthy life.  \n",
            "3  person desire healthy life.  \n",
            "4  person desire healthy life.  \n",
            "(7038, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc1GQh7yEm4C"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
        "\n",
        "        self.data = data  \n",
        "        #Initialize the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
        "\n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
        "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
        "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
        "\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_pair = self.tokenizer(sent1, sent2, padding='max_length', truncation=True, max_length=self.maxlen, return_tensors='pt')  \n",
        "        \n",
        "        token_ids = encoded_pair['input_ids'].squeeze(0) \n",
        "        attn_masks = encoded_pair['attention_mask'].squeeze(0) \n",
        "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  \n",
        "\n",
        "        if not self.with_labels: \n",
        "          return token_ids, attn_masks, token_type_ids            \n",
        "        else:\n",
        "          label = self.data.loc[index, 'label']\n",
        "          return token_ids, attn_masks, token_type_ids, label  \n",
        "            "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm0lAXvTZChm"
      },
      "source": [
        "class SentencePairClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
        "        super(SentencePairClassifier, self).__init__()\n",
        "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
        "        hidden_size = 768\n",
        "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    @autocast() \n",
        "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
        "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
        "        logits = self.cls_layer(self.dropout(pooler_output))\n",
        "        return logits"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3r8_npVf30D"
      },
      "source": [
        "def get_probs_from_logits(logits):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "def test_prediction(net, device, dataloader, with_labels=True, result_file=\"output.txt\"):\n",
        "    net.eval()\n",
        "    w = open(result_file, 'w')\n",
        "    probs_all = []\n",
        "\n",
        "    test_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "    test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "    dataloader = test_loader\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\n",
        "          seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "          logits = net(seq, attn_masks, token_type_ids)\n",
        "          probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "          probs_all += probs.tolist()\n",
        "    #print(len(probs_all))\n",
        "\n",
        "\n",
        "    w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
        "    probs_test = pd.read_csv(result_file, header=None)[0] \n",
        "    threshold = 0.5   # you can adjust this threshold for your own dataset\n",
        "    temp_list = []\n",
        "    #print(probs_all)\n",
        "    for entry in probs_all:\n",
        "      if entry >= threshold:\n",
        "        temp_list.append(1)\n",
        "      else:\n",
        "        temp_list.append(0)\n",
        "    df_val[\"temp_val\"] = temp_list\n",
        "    preds_test=df_val[\"temp_val\"].astype('uint8') # predicted labels using the above fixed threshold\n",
        "    labels_test = df_val[\"label\"].astype('uint8')\n",
        "\n",
        "    #print(preds_test)\n",
        "    #print(labels_test)\n",
        "\n",
        "    # Evaluation Method\n",
        "    metric = load_metric(\"glue\", \"mrpc\")\n",
        "    ans = metric._compute(predictions=preds_test, references=labels_test)\n",
        "    print(ans)\n",
        "    \n",
        "    w.close()\n",
        "    return ans['accuracy']"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl-rhuWsrg01"
      },
      "source": [
        "# print(\"Creation of the models' folder...\")\n",
        "# !mkdir models\n",
        "# !mkdir results"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-o6KyaFkU5u"
      },
      "source": [
        "def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
        "\n",
        "    besst_acc = 0\n",
        "    best_ep = 0\n",
        "\n",
        "\n",
        "    nb_iterations = len(train_loader)\n",
        "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
        "   \n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "            # Converting to cuda tensors\n",
        "            seq, attn_masks, token_type_ids, labels = seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "    \n",
        "            # Enables autocasting for the forward pass (model + loss)\n",
        "            with autocast():\n",
        "                # Compute Loss\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                loss = loss / iters_to_accumulate \n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (it + 1) % iters_to_accumulate == 0:\n",
        "                # Optimization step\n",
        "              \n",
        "                scaler.step(opti)\n",
        "                scaler.update()\n",
        "                lr_scheduler.step()\n",
        "                opti.zero_grad()\n",
        "        \n",
        "\n",
        "        val_acc = test_prediction(net, device, val_loader)\n",
        "        print(\"Accuracy = \"+ str(val_acc))\n",
        "        if val_acc > besst_acc:\n",
        "            print(\"Best epoch \" + str(ep))\n",
        "            net_copy = copy.deepcopy(net)  \n",
        "            besst_acc = val_acc\n",
        "            best_ep = ep + 1\n",
        "\n",
        "    # Saving the model\n",
        "    path_to_model='/content/drive/MyDrive/CSCI-544/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(besst_acc, 5), best_ep)\n",
        "    torch.save(net_copy.state_dict(), path_to_model)\n",
        "    #print(\"The model has been saved in {}\".format(path_to_model))\n",
        "\n",
        "    del loss\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6bzDp4FreS6"
      },
      "source": [
        "bert_model = \"albert-base-v2\"\n",
        "epochs = 7 \n",
        "maxlen = 128  \n",
        "bs = 16\n",
        "iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
        "lr = 2*1e-5 \n",
        "freeze_bert = False \n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZWGPomoryxy"
      },
      "source": [
        "# For Initial Training\n",
        "\n",
        "# # Creating instances of training and validation set\n",
        "# print(\"Reading training data...\")\n",
        "# train_set = CustomDataset(df_train, maxlen, bert_model)\n",
        "# print(\"Reading validation data...\")\n",
        "# val_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "# # Creating instances of training and validation dataloaders\n",
        "# train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
        "# val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
        "\n",
        "\n",
        "# if torch.cuda.device_count() > 0:  # if multiple GPUs\n",
        "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "#     #net = nn.DataParallel(net)\n",
        "\n",
        "# net.to(device)\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "# num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
        "# num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
        "# t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
        "# lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "# train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Copied From https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip -q install gputil\n",
        "# !pip -q install psutil\n",
        "# !pip -q install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ],
      "metadata": {
        "id": "ttzDyj71OACb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For Final Training\n",
        "\n",
        "# Creating instances of training and validation set\n",
        "train_set = CustomDataset(final_train_data, maxlen, bert_model)\n",
        "train_loader = DataLoader(train_set, batch_size=bs, num_workers=2)\n",
        "\n",
        "num_training_steps = epochs * len(train_loader) \n",
        "t_total = (len(train_loader) // iters_to_accumulate) * epochs \n",
        "\n",
        "\n",
        "# Creating instances of training and validation dataloaders\n",
        "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "val_loader = DataLoader(val_set, batch_size=bs, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = AdamW(net.parameters(), lr=lr, weight_decay=2e-2)\n",
        "\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_training_steps=t_total, num_warmup_steps=0 )\n",
        "\n",
        "train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XUz99AIEB4o",
        "outputId": "f73d5dbc-726f-4020-e140-354ce59f2f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 440/440 [01:10<00:00,  6.28it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 66/66 [00:04<00:00, 13.80it/s]\n",
            "Checking /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py for additional imports.\n",
            "Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6\n",
            "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.py\n",
            "Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/dataset_infos.json\n",
            "Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.885308056872038, 'f1': 0.8912848158131177}\n",
            "Accuracy = 0.885308056872038\n",
            "Best epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 440/440 [01:10<00:00,  6.28it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 66/66 [00:04<00:00, 13.92it/s]\n",
            "Checking /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py for additional imports.\n",
            "Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6\n",
            "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.py\n",
            "Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/dataset_infos.json\n",
            "Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.9232227488151659, 'f1': 0.9272237196765499}\n",
            "Accuracy = 0.9232227488151659\n",
            "Best epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▊        | 82/440 [00:13<00:56,  6.34it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_results(with_labels=True, result_file=\"/content/drive/MyDrive/CSCI-544/output.txt\"):\n",
        "    \n",
        "    net = SentencePairClassifier(bert_model)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net.load_state_dict(torch.load(\"/content/drive/MyDrive/CSCI-544/albert-base-v2_lr_2e-05_val_loss_0.06192_ep_7.pt\"))\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    \n",
        "    #w = open(result_file, 'w')\n",
        "    probs_all = []\n",
        "    rand_list = []\n",
        "    for _ in range(4850):\n",
        "      rand_list.append(0)\n",
        "    df_test['label'] = rand_list\n",
        "\n",
        "    test_set = CustomDataset(df_test, maxlen, bert_model)\n",
        "    test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "    dataloader = test_loader\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\n",
        "          seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "          logits = net(seq, attn_masks, token_type_ids)\n",
        "          probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "          probs_all += probs.tolist()\n",
        "       \n",
        "    #print(len(probs_all))\n",
        "    #w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
        "    #probs_test = pd.read_csv(result_file, header=None)[0]  # prediction probabilities\n",
        "    threshold = 0.5   # you can adjust this threshold for your own dataset\n",
        "    temp_list = []\n",
        "    #print(probs_all)\n",
        "    for entry in probs_all:\n",
        "      if entry >= threshold:\n",
        "        temp_list.append(1)\n",
        "      else:\n",
        "        temp_list.append(0)\n",
        "    #df_val[\"temp_val\"] = temp_list\n",
        "    #preds_test=df_val[\"temp_val\"].astype('uint8') # predicted labels using the above fixed threshold\n",
        "    #labels_test = df_val[\"label\"].astype('uint8')\n",
        "    #print(preds_test)\n",
        "    #print(labels_test)\n",
        "    #metric = load_metric(\"glue\", \"mrpc\")\n",
        "    #print(metric._compute(predictions=preds_test, references=labels_test))\n",
        "    #print(metric)\n",
        "    results = [int(x) for x in temp_list]\n",
        "    assert (len(results) == 4850)\n",
        "    # write your prediction results to 'upload_predictions.txt' and upload that later\n",
        "    with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
        "        for x in results:\n",
        "            fp.write(str(x) + '\\n')\n",
        "    #w.close()\n",
        "\n",
        "find_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsOBN9eB6jKi",
        "outputId": "2d543a97-34f3-4e6b-de54-e64d6ca8bcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 304/304 [00:19<00:00, 15.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For testing on dev set to make sure no shuffle\n",
        "\n",
        "# def find_results(with_labels=True, result_file=\"/content/drive/MyDrive/CSCI-544/output.txt\"):\n",
        "#     \"\"\"\n",
        "#     Predict the probabilities on a dataset with or without labels and print the result in a file\n",
        "#     \"\"\"\n",
        "#     net = SentencePairClassifier(bert_model)\n",
        "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     net.load_state_dict(torch.load(\"/content/drive/MyDrive/CSCI-544/albert-base-v2_lr_2e-05_val_loss_0.06192_ep_7.pt\"))\n",
        "#     net.to(device)\n",
        "#     net.eval()\n",
        "#     #w = open(result_file, 'w')\n",
        "#     probs_all = []\n",
        "#     # rand_list = []\n",
        "#     # for _ in range(4850):\n",
        "#     #   rand_list.append(0)\n",
        "#     # df_test['label'] = rand_list\n",
        "\n",
        "#     test_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "#     test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "#     dataloader = test_loader\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         if with_labels:\n",
        "#             for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\n",
        "#                 seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "#                 logits = net(seq, attn_masks, token_type_ids)\n",
        "#                 probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "#                 probs_all += probs.tolist()\n",
        "#         else:\n",
        "#             for seq, attn_masks, token_type_ids in tqdm(dataloader):\n",
        "#                 seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "#                 logits = net(seq, attn_masks, token_type_ids)\n",
        "#                 probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "#                 probs_all += probs.tolist()\n",
        "#     #print(len(probs_all))\n",
        "#     #w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
        "#     #probs_test = pd.read_csv(result_file, header=None)[0]  # prediction probabilities\n",
        "#     threshold = 0.5   # you can adjust this threshold for your own dataset\n",
        "#     temp_list = []\n",
        "#     #print(probs_all)\n",
        "#     for entry in probs_all:\n",
        "#       if entry >= threshold:\n",
        "#         temp_list.append(1)\n",
        "#       else:\n",
        "#         temp_list.append(0)\n",
        "#     df_val[\"temp_val\"] = temp_list\n",
        "#     preds_test=df_val[\"temp_val\"].astype('uint8') # predicted labels using the above fixed threshold\n",
        "#     labels_test = df_val[\"label\"].astype('uint8')\n",
        "#     print(preds_test)\n",
        "#     print(labels_test)\n",
        "#     metric = load_metric(\"glue\", \"mrpc\")\n",
        "#     print(metric._compute(predictions=preds_test, references=labels_test))\n",
        "#     print(metric)\n",
        "#     # results = [int(x) for x in temp_list]\n",
        "#     # assert (len(results) == 4850)\n",
        "#     # # write your prediction results to 'upload_predictions.txt' and upload that later\n",
        "#     # with open('upload_predictions2.txt', 'w', encoding = 'utf-8') as fp:\n",
        "#     #     for x in results:\n",
        "#     #         fp.write(str(x) + '\\n')\n",
        "#     #w.close()\n",
        "\n",
        "# find_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTvrvmAUBqag",
        "outputId": "5d627a22-d985-43fe-a630-80e3231a1047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 66/66 [00:04<00:00, 14.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       1\n",
            "1       0\n",
            "2       1\n",
            "3       0\n",
            "4       1\n",
            "       ..\n",
            "1050    0\n",
            "1051    0\n",
            "1052    1\n",
            "1053    1\n",
            "1054    1\n",
            "Name: temp_val, Length: 1055, dtype: uint8\n",
            "0       1\n",
            "1       0\n",
            "2       1\n",
            "3       0\n",
            "4       1\n",
            "       ..\n",
            "1050    0\n",
            "1051    0\n",
            "1052    1\n",
            "1053    1\n",
            "1054    1\n",
            "Name: label, Length: 1055, dtype: uint8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py for additional imports.\n",
            "Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6\n",
            "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.py\n",
            "Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/dataset_infos.json\n",
            "Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.9848341232227488, 'f1': 0.9855334538878843}\n",
            "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
            "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
            "Args:\n",
            "    predictions: list of translations to score.\n",
            "        Each translation should be tokenized into a list of tokens.\n",
            "    references: list of lists of references for each translation.\n",
            "        Each reference should be tokenized into a list of tokens.\n",
            "Returns: depending on the GLUE subset, one or several of:\n",
            "    \"accuracy\": Accuracy\n",
            "    \"f1\": F1\n",
            "    \"pearson\": Pearson Correlation\n",
            "    \"spearmanr\": Spearman Correlation\n",
            "    \"matthews_correlation\": Matthew Correlation\n",
            "\"\"\", stored examples: 0)\n"
          ]
        }
      ]
    }
  ]
}