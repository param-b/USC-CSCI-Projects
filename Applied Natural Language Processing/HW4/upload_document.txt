Natural Language Inference. Entailing or Neutral 

### Method Name ###
Use a phrasal name to describe your method, e.g. training a BiLSTM cross-encoder from scratch, fine-tuning RoBERTa-large-MNLI, etc.
Fine Tuning of Albert-base-v2

### Sentence pair encoder ###
Use up to 5 sentences to describe your encoder for the sentence pairs. Need to mention the following:
- Is it a bi-encoder or cross-encoder?
- What type of encoder (LSTM, Transformer, etc.)
- Is it based on a pre-trained model (BERT-large? RoBERTa-large-SNLI? BART-large-MNLI?) or completely trained from scratch by yourself (then how do you chracterize the words and aggregate them into sentence representations)?
The model is am transformer.
The base model is an Albert Base v2 from the Huggingface repository.
The encoding is using the Albert Tokenizer with [CLS] and [SEP] with max length at 128
The base model was initially trained on sentence pair classification task.
Spell correction used in pre-processing

### Training & Development ###
Up to 5 sentences: how did you evaluate your solution using the dev set before submitting to the leaderboard? What are some key hyperparameter values (e.g., optimizer, learning rate, batch size, etc.)? How did you terminate the training (using a fixed #epochs, early stopping based on dev set performance)?
Evaluation done on loss and accuracy.
Batch size 16 but effectively 32 as update occurred over interval of 16
AdamW Optimizer used with learning rate at 0.00002
Training was done for 7 epoch to prevent overfitting.
Learning rate decreased linearly.


### Other methods ###
Did you try other methods than the submitted one?
Bert-Base-Uncased Transformer.

### Packages ###
List the key python packages you have used in this assignment.
Huggingface Transformer
PyTorch
Numpy
Pandas