{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqS0HeNOsDkv"
      },
      "source": [
        "### Read training, dev and unlabeled test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCoST412sDk5"
      },
      "source": [
        "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CorEFMMisSYi",
        "outputId": "247a54de-b5c0-489a-f8ea-6e8884121608"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6csbt0Dk0U_D"
      },
      "outputs": [],
      "source": [
        "# import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer\n",
        "\t\n",
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0VglzTpsDk7"
      },
      "outputs": [],
      "source": [
        "train, dev, test = [], [], []\n",
        "train_text, dev_text, test_text = [], [], []\n",
        "train_label, dev_label, test_label = [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHfiNdIqsDk_",
        "outputId": "6a11a641-8491-43e2-e3b5-2d40ef7c8064"
      },
      "outputs": [],
      "source": [
        "for x in open('/content/drive/MyDrive/CSCI-544/train_enc.tsv', encoding='utf-8'):\n",
        "    x = x.rstrip('\\n\\r').split('\\t')\n",
        "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
        "    x[0] = int(x[0]) \n",
        "    train.append(x)\n",
        "    train_text.append(x[1].lower())\n",
        "    train_label.append(x[0])\n",
        "print (len(train))\n",
        "print (train[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZmSYl39sDlC",
        "outputId": "a4df4c61-cba5-481c-c727-96620b2d20f9"
      },
      "outputs": [],
      "source": [
        "for x in open('/content/drive/MyDrive/CSCI-544/dev_enc.tsv', encoding='utf-8'):\n",
        "    x = x.rstrip('\\n\\r').split('\\t')\n",
        "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
        "    x[0] = int(x[0]) \n",
        "    dev.append(x)\n",
        "    dev_text.append(x[1].lower())\n",
        "    dev_label.append(x[0])\n",
        "print (len(dev))\n",
        "print (dev[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7ib0rosDlE"
      },
      "source": [
        "#### Different from 'train' and 'dev' that are both list of tuples, 'test' will be just a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiMglEW3sDlF",
        "outputId": "bdd0e9d3-cabd-41cb-b583-91f97323ca1b"
      },
      "outputs": [],
      "source": [
        "for x in open('/content/drive/MyDrive/CSCI-544/test_enc_unlabeled.tsv', encoding='utf-8'):\n",
        "    x = x.rstrip('\\n\\r').lower()\n",
        "    test.append(x)\n",
        "print (len(test))\n",
        "print (test[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNMcN5BDsDlH"
      },
      "source": [
        "#### You can split every sentence into lists of words by white spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-YCaYmlsDlK"
      },
      "outputs": [],
      "source": [
        "train_split = [[x[0], x[1].lower().split(' ')] for x in train]\n",
        "dev_split = [[x[0], x[1].lower().split(' ')] for x in dev]\n",
        "test_split = [[x.split(' ')] for x in test]\n",
        "#test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbe9hqYRsDlN"
      },
      "source": [
        "### Main Code Body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c4O1ghxsDlP"
      },
      "source": [
        "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ro1r-xA2B0L"
      },
      "source": [
        "Training of Embeddings happens here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLfZEphSypXl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_text(list_sentences, lower=False):\n",
        "    comments = []\n",
        "    for text in list_sentences:\n",
        "        #print(text)\n",
        "        single_sentence = ' '.join(text).strip().lower()\n",
        "        temp_text = single_sentence.replace('.', '')\n",
        "        text = temp_text.replace(',', '')\n",
        "        txt = tk.tokenize(text)\n",
        "        comments.append(txt)\n",
        "    return comments\n",
        "\n",
        "sentences = []\n",
        "for a, b in train_split:\n",
        "  #print(b)\n",
        "  sentences.append(b)\n",
        "  # if 'úol' in b:\n",
        "  #   print(\"There\")\n",
        "for a, b in dev_split:\n",
        "  sentences.append(b)\n",
        "  # if 'úol' in b:\n",
        "  #   print(\"There\")\n",
        "for a in test_split:\n",
        "  #print(a[0])\n",
        "  sentences.append(a[0])\n",
        "  # if 'úol' in a:\n",
        "  #   print(\"There\")\n",
        "#print(sentences[1])\n",
        "#print(sentences[-1])\n",
        "\n",
        "new_sentence = process_text(sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F83aNhHdMg0W",
        "outputId": "2c08b235-717c-4fd7-ed67-73e1e8348949"
      },
      "outputs": [],
      "source": [
        "print(len(new_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipacQ-YkwHWk",
        "outputId": "20877d4f-de67-4984-e009-057d0bee2b31"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "# define training data\n",
        "# train model\n",
        "model = Word2Vec(new_sentence, window=5,min_count=5,negative=15,iter=10)\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print(words)\n",
        "# access vector for one word\n",
        "print(model['cêêö'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cfiCaP4-Jfg",
        "outputId": "45ccc2c9-c9cb-4160-b6c9-3ca6ecda61dc"
      },
      "outputs": [],
      "source": [
        "print(\"Number of word vectors: {}\".format(len(model.wv.vocab)))\n",
        "print(model.wv.vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh2K2b9WAT0O",
        "outputId": "0d6e0886-efc1-4231-c29b-ad4569205773"
      },
      "outputs": [],
      "source": [
        "MAX_NB_WORDS = len(model.wv.vocab)\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "word_index = {t: i+1 for i,t in enumerate(model.wv.vocab)}\n",
        "\n",
        "print(len(word_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFoLOZ3iOL1w"
      },
      "source": [
        "Fasttext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5SFS2StOOR-",
        "outputId": "245a6df5-3c21-410f-a130-1ce7a6eb82eb"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "model2 = FastText(size=100, window=3, min_count=1, sentences=new_sentence, iter=10)\n",
        "print(\"Number of word vectors: {}\".format(len(model2.wv.vocab)))\n",
        "print(model2.wv.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqiYcmCrP7nX",
        "outputId": "d7ca8a5a-3f14-4de9-d7e6-d2dab30f55b8"
      },
      "outputs": [],
      "source": [
        "MAX_NB_WORDS = len(model2.wv.vocab)\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "word_index_2 = {t: i+1 for i,t in enumerate(model2.wv.vocab)}\n",
        "\n",
        "print(len(word_index_2))\n",
        "print(model2['lkêcê'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmDhm3me2QnQ"
      },
      "source": [
        "Actual work with data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcX8E7nisDlQ"
      },
      "outputs": [],
      "source": [
        "# from nltk.probability import FreqDist\n",
        "# import re\n",
        "# stopList = []\n",
        "# for a, b in train_split:\n",
        "#   for word in b:\n",
        "#     stopList.append(word)\n",
        "    \n",
        "# fdist = FreqDist(stopList)\n",
        "# top_ten = fdist.most_common(15)\n",
        "# top_ten_words = [x for x, y in top_ten]\n",
        "# def remove_stop_words(old_data):\n",
        "#   temp_list_new = []\n",
        "#   temp_list_labels = []\n",
        "#   for a, sentence in old_data:\n",
        "#     temp_list = []\n",
        "#     #sentence_new = clean_str(' '.join(sentence))\n",
        "#     sentence_new = sentence\n",
        "#     for word in sentence_new:\n",
        "#       if word not in top_ten_words:\n",
        "#         #print(word)\n",
        "#         temp_list.append(word)\n",
        "#       # if word == 'úol':\n",
        "#       #   print(\"There\")\n",
        "#     temp_list_sent = \" \".join(temp_list).strip()\n",
        "#     temp_list_new.append(temp_list_sent)\n",
        "#     temp_list_labels.append(a)\n",
        "#   return temp_list_new, temp_list_labels\n",
        "# # Tokenization/string cleaning for dataset\n",
        "# def clean_str(inp):\n",
        "#     new_inp = re.sub(r'[^\\w\\s]', '', inp)\n",
        "#     return new_inp.strip().lower().split(\" \")\n",
        "# #print(top_ten)\n",
        "# #\n",
        "# #print(top_ten_words)\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "import re\n",
        "stopList = []\n",
        "for a, b in train_split:\n",
        "  for word in b:\n",
        "    stopList.append(word)\n",
        "    \n",
        "fdist = FreqDist(stopList)\n",
        "top_ten = fdist.most_common(15)\n",
        "top_ten_words = [x for x, y in top_ten]\n",
        "def remove_stop_words(old_data, word_index_local):\n",
        "  temp_list_new = []\n",
        "  temp_list_labels = []\n",
        "  ctr = 0\n",
        "  for a, sentence in old_data:\n",
        "    temp_list = []\n",
        "    #sentence_new = clean_str(' '.join(sentence))\n",
        "    sentence_new = sentence\n",
        "    for word in sentence_new:\n",
        "      if word not in top_ten_words:\n",
        "        #print(word)\n",
        "        val = word_index_local.get(word, 0)\n",
        "        if val == 0:\n",
        "          ctr += 1\n",
        "          #print(word)\n",
        "        temp_list.append(word_index_local.get(word, 0))\n",
        "      # if word == 'úol':\n",
        "      #   print(\"There\")\n",
        "    temp_list_new.append(temp_list)\n",
        "    temp_list_labels.append(a)\n",
        "  print(ctr)\n",
        "  return temp_list_new, temp_list_labels\n",
        "# Tokenization/string cleaning for dataset\n",
        "def clean_str(inp):\n",
        "    new_inp = re.sub(r'[^\\w\\s]', '', inp)\n",
        "    return new_inp.strip().lower().split(\" \")\n",
        "#print(top_ten)\n",
        "#\n",
        "#print(top_ten_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnKadztlIjcg",
        "outputId": "4f133b4b-0cc8-44ff-8fff-a28a8a78112e"
      },
      "outputs": [],
      "source": [
        "print(dev_split)\n",
        "train_text_new, train_label_new = remove_stop_words(train_split, word_index)\n",
        "dev_text_new, dev_label_new = remove_stop_words(dev_split, word_index)\n",
        "print(dev_text_new)\n",
        "# stopList2 = []\n",
        "# for a, b in train_split_new:\n",
        "#   for word in b:\n",
        "#     stopList2.append(word)\n",
        "# fdist2 = FreqDist(stopList2)\n",
        "# top_ten_2 = fdist2.most_common(10)\n",
        "# print(top_ten_2)\n",
        "# print(len(train_split_new))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsGNUY0Ze_Mc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras import initializers\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from nltk import tokenize\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucirkVB9T2QJ",
        "outputId": "a604ef5e-5d62-41dc-9457-e135067a41dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "x_train = np.expand_dims(np.array(pad_sequences(train_text_new, maxlen=MAX_SEQUENCE_LENGTH, \n",
        "                     padding=\"post\", truncating=\"post\")), axis=1)\n",
        "y_train = to_categorical(np.array(train_label_new))\n",
        "x_val = np.expand_dims(pad_sequences(dev_text_new, maxlen=MAX_SEQUENCE_LENGTH, \n",
        "                     padding=\"post\", truncating=\"post\"), axis=1)\n",
        "y_val = to_categorical(np.array(dev_label_new))\n",
        "print(x_train.shape)\n",
        "print([x_train[0]])\n",
        "print(x_val.shape)\n",
        "print(type(y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crPVnqKiP0-Y",
        "outputId": "74ca71c9-420b-48c0-a30d-4cc917e00388"
      },
      "outputs": [],
      "source": [
        "print(dev_split)\n",
        "train_text_new_2, train_label_new_2 = remove_stop_words(train_split, word_index_2)\n",
        "dev_text_new_2, dev_label_new_2 = remove_stop_words(dev_split, word_index_2)\n",
        "print(dev_text_new_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn_eDb41UDL6"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "x_train_2 = np.expand_dims(pad_sequences(train_text_new, maxlen=MAX_SEQUENCE_LENGTH, \n",
        "                     padding=\"post\", truncating=\"post\"), axis=1)\n",
        "x_val_2 = np.expand_dims(pad_sequences(dev_text_new_2, maxlen=MAX_SEQUENCE_LENGTH, \n",
        "                     padding=\"post\", truncating=\"post\"), axis=1)\n",
        "\n",
        "y_train_2= np.array(to_categorical(train_label_new_2))\n",
        "y_val_2 = np.array(to_categorical(dev_label_new_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8NzZf7EqtTj"
      },
      "outputs": [],
      "source": [
        "# Implementation of Hierarchical Attentional Networks for Document Classification\n",
        "# http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
        "\n",
        "\n",
        "maxlen = MAX_SEQUENCE_LENGTH\n",
        "max_sentences = 1\n",
        "max_words = 50000\n",
        "embedding_dim = 100\n",
        "# validation_split = 0.2\n",
        "reviews = []\n",
        "labels = []\n",
        "texts = []\n",
        "# glove_dir = \"./glove.6B\"\n",
        "embeddings_index = {}\n",
        "\n",
        "\n",
        "# class defining the custom attention layer\n",
        "class HierarchicalAttentionNetwork(Layer):\n",
        "    def __init__(self, attention_dim):\n",
        "        #self.init = initializers.get('normal')\n",
        "        self.init = initializers.get('glorot_normal')\n",
        "        self.supports_masking = False\n",
        "        self.attention_dim = attention_dim\n",
        "        super(HierarchicalAttentionNetwork, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
        "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
        "        self.b = K.variable(self.init((self.attention_dim,)))\n",
        "        self._trainable_weights = [self.W, self.b, self.u]\n",
        "        super(HierarchicalAttentionNetwork, self).build(input_shape)\n",
        "\n",
        "    # def compute_mask(self, inputs, mask=None):\n",
        "    #     return mask\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "       \n",
        "        attention_input = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
        "        attenition_op = K.exp(K.squeeze(K.dot(attention_input, self.u), -1))\n",
        "\n",
        "        # Helps in finding attention of weight in the sentence\n",
        "        attenition_op /= K.cast(K.sum(attenition_op, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        weighted_input = x * K.expand_dims(attenition_op)\n",
        "        output = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "# input_data = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
        "\n",
        "# for idx in range(input_data.review.shape[0]):\n",
        "#     text = BeautifulSoup(input_data.review[idx], features=\"html5lib\")\n",
        "#     text = clean_str(text.get_text().encode('ascii', 'ignore'))\n",
        "#     texts.append(text)\n",
        "#     sentences = tokenize.sent_tokenize(text)\n",
        "#     reviews.append(sentences)\n",
        "#     labels.append(input_data.sentiment[idx])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDudutBfyO9Q"
      },
      "outputs": [],
      "source": [
        "# # Tokenization Takes place here\n",
        "# # Create complete train + dev\n",
        "# texts = train_text_new + dev_text_new\n",
        "\n",
        "# tokenizer = Tokenizer(num_words=max_words, filters=']})\\t\\n', char_level=False)\n",
        "# tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "# print('Total %s unique tokens.' % len(word_index))\n",
        "\n",
        "# def tokenize_each_dataset(reviews):\n",
        "#   data = np.zeros((len(texts), max_sentences, maxlen), dtype='int32')\n",
        "#   for i, sentences in enumerate(reviews):\n",
        "#     #print(sentences)\n",
        "#     j = 0\n",
        "#     sent = sentences\n",
        "#     if j < max_sentences:\n",
        "#       wordTokens = text_to_word_sequence(sent, filters=']})\\t\\n')\n",
        "#       #print(wordTokens)\n",
        "#       k = 0\n",
        "#       for _, word in enumerate(wordTokens):\n",
        "#         if word == 'úêê':\n",
        "#           print('there' + word)\n",
        "#           print(wordTokens)\n",
        "#           print(sent)\n",
        "#         if k < maxlen and tokenizer.word_index[word] < max_words:\n",
        "#           #print(j)\n",
        "#           data[i, j, k] = tokenizer.word_index[word]\n",
        "#           k = k + 1\n",
        "#   return data\n",
        "\n",
        "# # labels = to_categorical(np.asarray(labels))\n",
        "# # print('Shape of reviews (data) tensor:', data.shape)\n",
        "# # print('Shape of sentiment (label) tensor:', labels.shape)\n",
        "\n",
        "# # Tokenize dev and train set using x_val, y_val\n",
        "# x_train = tokenize_each_dataset(train_text_new)\n",
        "# y_train = to_categorical(np.asarray(train_label_new))\n",
        "\n",
        "# x_val = tokenize_each_dataset(dev_text_new)\n",
        "# y_val = to_categorical(np.asarray(dev_label_new))\n",
        "\n",
        "# print(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAoa_0L2yx5R",
        "outputId": "4b1640a9-ebe5-49f8-d538-18b6693e85f6"
      },
      "outputs": [],
      "source": [
        "# Build the Embedding layer here\n",
        "def get_weight_matrix(vocab, model_local):\n",
        "    total = 0\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with random \n",
        "    embedding_matrix = np.random.random((vocab_size, embedding_dim))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        if len(word.strip()) > 0:\n",
        "          try:\n",
        "            embedding_matrix[i] = model_local.wv.get_vector(word.strip())\n",
        "          except KeyError as e:\n",
        "            total += 1\n",
        "            #print(e)\n",
        "    print(total)\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = get_weight_matrix(word_index, model)\n",
        "embedding_matrix_2 = get_weight_matrix(word_index_2, model2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "oiAoRw0B4wct",
        "outputId": "3cc42faa-4930-4a5e-de6d-db6c46a709d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# indices = np.arange(data.shape[0])\n",
        "# np.random.shuffle(indices)\n",
        "# data = data[indices]\n",
        "# labels = labels[indices]\n",
        "# nb_validation_samples = int(validation_split * data.shape[0])\n",
        "\n",
        "# x_train = data[:-nb_validation_samples]\n",
        "# y_train = labels[:-nb_validation_samples]\n",
        "# x_val = data[-nb_validation_samples:]\n",
        "# y_val = labels[-nb_validation_samples:]\n",
        "\n",
        "# print('Number of positive and negative reviews in training and validation set')\n",
        "# print y_train.sum(axis=0)\n",
        "# print y_val.sum(axis=0)\n",
        "\n",
        "\n",
        "# f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Total %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# # building Hierachical Attention network\n",
        "# embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(maxlen)\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True)\n",
        "\n",
        "def word_attention():\n",
        "  sentence_input = Input(shape=(maxlen,), dtype='int32')\n",
        "  sequence_emded_new = embedding_layer(sentence_input)\n",
        "  fn_lstm_word = Bidirectional(GRU(100, return_sequences=True))(sequence_emded_new)\n",
        "  fn_attn_word = HierarchicalAttentionNetwork(100)(fn_lstm_word)\n",
        "  sentenceEncoder = Model(sentence_input, fn_attn_word)\n",
        "  return sentenceEncoder\n",
        "\n",
        "def sentence_attention(sentenceEncoder):\n",
        "  complete_input = Input(shape=(max_sentences, maxlen), dtype='int32')\n",
        "  review_encoder = TimeDistributed(sentenceEncoder)(complete_input)\n",
        "  fn_lstm = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
        "  fn_attn = HierarchicalAttentionNetwork(100)(fn_lstm)\n",
        "  compare_preds = Dense(2, activation='softmax')(fn_attn)\n",
        "  model_han = Model(complete_input, compare_preds)\n",
        "  return model_han\n",
        "\n",
        "sentence_encode =  word_attention()\n",
        "model_final = sentence_attention(sentenceEncoder=sentence_encode)\n",
        "model_final.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.0001)\n",
        "\n",
        "#print(\"model fitting - Hierachical attention network\")\n",
        "print('Trial Fitting.')\n",
        "#model_final.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20, batch_size=100, callbacks=[reduce_lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgVdYeozpUHQ"
      },
      "source": [
        "For Final Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_lQGknCpKa4",
        "outputId": "14555e20-0536-4977-e0e5-a490ed344648"
      },
      "outputs": [],
      "source": [
        "# For Final Training\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,patience=5, min_lr=0.00001)\n",
        "x_final = np.concatenate((x_train, x_val), axis=0)\n",
        "y_final = np.concatenate((y_train, y_val), axis=0)\n",
        "print(x_final.shape)\n",
        "print(y_final.shape)\n",
        "model_final.fit(x_final, y_final, validation_data=(x_val, y_val), epochs=20, batch_size=32, callbacks=[reduce_lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC7sFchzscBs"
      },
      "source": [
        "Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kKmgqdBu9vu"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words_test(old_data, word_index_local):\n",
        "  temp_list_new = []\n",
        "  temp_list_labels = []\n",
        "  ctr = 0\n",
        "  for sentence in old_data:\n",
        "    temp_list = []\n",
        "    #sentence_new = clean_str(' '.join(sentence))\n",
        "    sentence_new = sentence\n",
        "    for word in sentence_new:\n",
        "      if word not in top_ten_words:\n",
        "        #print(word)\n",
        "        val = word_index_local.get(word, 0)\n",
        "        if val == 0:\n",
        "          ctr += 1\n",
        "          #print(word)\n",
        "        temp_list.append(word_index_local.get(word, 0))\n",
        "      # if word == 'úol':\n",
        "      #   print(\"There\")\n",
        "    temp_list_new.append(temp_list)\n",
        "  print(ctr)\n",
        "  return temp_list_new\n",
        "\n",
        "def process_text_test(list_sentences):\n",
        "  comments = []\n",
        "  ctr = 0\n",
        "  for text in list_sentences:\n",
        "    ctr += 1\n",
        "    #print(text[0])\n",
        "    text_curr = text[0]\n",
        "    single_sentence = ' '.join(text_curr).strip().lower()\n",
        "    temp_text = single_sentence.replace('.', '')\n",
        "    text = temp_text.replace(',', '')\n",
        "    txt = tk.tokenize(text)\n",
        "    comments.append(txt)\n",
        "  print(\"Entries \"+ctr)\n",
        "  return comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3EsbQ0NsaN7",
        "outputId": "9b87b421-9c77-4750-da6e-2e9a2447554e"
      },
      "outputs": [],
      "source": [
        "test_split_processed = process_text_test(test_split)\n",
        "test_split_new = remove_stop_words_test(test_split_processed, word_index)\n",
        "x_pred = np.expand_dims(pad_sequences(test_split_new, maxlen=MAX_SEQUENCE_LENGTH, \n",
        "                     padding=\"post\", truncating=\"post\"), axis=1)\n",
        "\n",
        "y_pred = model_final.predict(x_pred)\n",
        "print(y_pred.shape)\n",
        "y_classes = y_pred.argmax(axis=-1)\n",
        "print(y_classes.shape)\n",
        "print(y_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "v98p8mQIu4X_",
        "outputId": "137ff9a6-bdea-4f20-a3e5-c28da737d0c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # indices = np.arange(data.shape[0])\n",
        "# # np.random.shuffle(indices)\n",
        "# # data = data[indices]\n",
        "# # labels = labels[indices]\n",
        "# # nb_validation_samples = int(validation_split * data.shape[0])\n",
        "\n",
        "# # x_train = data[:-nb_validation_samples]\n",
        "# # y_train = labels[:-nb_validation_samples]\n",
        "# # x_val = data[-nb_validation_samples:]\n",
        "# # y_val = labels[-nb_validation_samples:]\n",
        "\n",
        "# # print('Number of positive and negative reviews in training and validation set')\n",
        "# # print y_train.sum(axis=0)\n",
        "# # print y_val.sum(axis=0)\n",
        "\n",
        "\n",
        "# # f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "# # for line in f:\n",
        "# #     values = line.split()\n",
        "# #     word = values[0]\n",
        "# #     coefs = np.asarray(values[1:], dtype='float32')\n",
        "# #     embeddings_index[word] = coefs\n",
        "# # f.close()\n",
        "\n",
        "# # print('Total %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# # # building Hierachical Attention network\n",
        "# # embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
        "# # for word, i in word_index.items():\n",
        "# #     embedding_vector = embeddings_index.get(word)\n",
        "# #     if embedding_vector is not None:\n",
        "# #         # words not found in embedding index will be all-zeros.\n",
        "# #         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# print(maxlen)\n",
        "\n",
        "# embedding_layer = Embedding(len(word_index_2) + 1, embedding_dim, weights=[embedding_matrix_2], input_length=maxlen, trainable=True)\n",
        "\n",
        "# sentence_input = Input(shape=(maxlen,), dtype='int32')\n",
        "# embedded_sequences = embedding_layer(sentence_input)\n",
        "# lstm_word = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
        "# attn_word = HierarchicalAttentionNetwork(100)(lstm_word)\n",
        "# sentenceEncoder = Model(sentence_input, attn_word)\n",
        "\n",
        "# review_input = Input(shape=(max_sentences, maxlen), dtype='int32')\n",
        "# review_encoder = TimeDistributed(sentenceEncoder)(review_input)\n",
        "# lstm_sentence = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
        "# attn_sentence = HierarchicalAttentionNetwork(100)(lstm_sentence)\n",
        "# preds = Dense(2, activation='softmax')(attn_sentence)\n",
        "# model_final = Model(review_input, preds)\n",
        "\n",
        "# model_final.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# print(\"model fitting - Hierachical attention network\")\n",
        "# model_final.fit(x_train_2, y_train_2, validation_data=(x_val_2, y_val_2), epochs=10, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWCkZy7vsDlS"
      },
      "outputs": [],
      "source": [
        "# Eventually, results need to be a list of 2028 0 or 1's\n",
        "results = y_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWXnB8FsDlT"
      },
      "source": [
        "### Output Prediction Result File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MpLWWrLsDlU"
      },
      "source": [
        "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js3EjgwRsDlV"
      },
      "outputs": [],
      "source": [
        "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
        "#those results are in the list called 'results'\n",
        "assert (len(results) == 2028)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9Wq2xVwsDlW"
      },
      "outputs": [],
      "source": [
        "# make sure the results are not float numbers, but intergers 0 and 1\n",
        "results = [int(x) for x in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_czDBK9sDlX"
      },
      "outputs": [],
      "source": [
        "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
        "with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
        "    for x in results:\n",
        "        fp.write(str(x) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Er1kWiyvnI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JTWXnB8FsDlT"
      ],
      "name": "Copy of CSCI-HW3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
